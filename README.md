# ğŸ­ Multimodal Video Sentiment Analysis SaaS

Production-ready sentiment and emotion detection from video using multi-modal deep learning. Built with PyTorch, AWS SageMaker, and Next.js 15.


> **ğŸ“¦ Project Structure:** This repository contains two main directories:
> - `sentiment-model/` - ML training, model artifacts, and SageMaker deployment
> - `video-sentiment-saas/` - Next.js SaaS application with frontend and API

## ğŸ¯ Features

- **ğŸ¬ Video Analysis**: Upload videos and get per-utterance emotion & sentiment scores
- **ğŸ§  Multi-Modal AI**: Combines video frames (ResNet3D), audio (CNN), and text (RoBERTa + DistilBERT)
- **ğŸ“Š 7 Emotions + 3 Sentiments**: Detects anger, disgust, fear, joy, neutral, sadness, surprise + positive/negative/neutral sentiment
- **â˜ï¸ Cloud Production**: Deployed on AWS SageMaker with GPU inference (ml.g5.xlarge)
- **ğŸ” SaaS Ready**: User authentication, API quotas, S3 storage, async processing
- **âš¡ Real-time**: Async polling architecture for smooth UX with long-running inference

## ğŸ—ï¸ Tech Stack

**Machine Learning:**
- PyTorch 2.5.1
- Whisper (audio transcription)
- RoBERTa (emotion classification)
- DistilBERT (sentiment analysis)
- Late fusion architecture

**Cloud Infrastructure:**
- AWS SageMaker (training + inference)
- AWS S3 (video storage)
- CloudWatch (logging)

**SaaS Application:**
- Next.js 15 (App Router)
- NextAuth.js (authentication)
- Prisma ORM
- PostgreSQL database
- Tailwind CSS + shadcn/ui

## ğŸš€ Live Demo

**[https://multimodal-ai-saas.vercel.app](https://multimodal-ai-saas.vercel.app)**

Try it: Upload a video â†’ Get detailed emotion analysis for each spoken utterance

## ğŸ“ Project Structure

```
â”œâ”€â”€ sentiment-model/               # ML Training & Deployment
â”‚   â”œâ”€â”€ dataset/                   # MELD dataset (train/dev/test)
â”‚   â”œâ”€â”€ training/                  # Model training code
â”‚   â”‚   â”œâ”€â”€ meld_dataset.py        # PyTorch dataset loader
â”‚   â”‚   â”œâ”€â”€ models.py              # Multi-modal architecture
â”‚   â”‚   â”œâ”€â”€ train.py               # Training script
â”‚   â”‚   â””â”€â”€ requirements.txt
â”‚   â”œâ”€â”€ deployment/                # SageMaker deployment
â”‚   â”‚   â”œâ”€â”€ inference.py           # Endpoint handler (Whisper + RoBERTa + DistilBERT)
â”‚   â”‚   â”œâ”€â”€ models.py              # Model definitions
â”‚   â”‚   â”œâ”€â”€ requirements.txt       # Inference dependencies
â”‚   â”‚   â”œâ”€â”€ model.tar.gz           # Packaged model artifact
â”‚   â”‚   â”œâ”€â”€ create_model_package.py
â”‚   â”‚   â””â”€â”€ deploy_endpoint.py
â”‚   â”œâ”€â”€ train_sagemaker.py         # SageMaker training job launcher
â”‚   â””â”€â”€ MELD.Raw.tar.gz            # Raw dataset archive
â”‚
â””â”€â”€ video-sentiment-saas/          # Next.js SaaS Application
    â”œâ”€â”€ src/
    â”‚   â”œâ”€â”€ app/
    â”‚   â”‚   â”œâ”€â”€ api/               # API Routes
    â”‚   â”‚   â”‚   â”œâ”€â”€ direct-upload/     # S3 upload handler
    â”‚   â”‚   â”‚   â”œâ”€â”€ start-analysis/    # Async inference trigger
    â”‚   â”‚   â”‚   â”œâ”€â”€ analysis-status/   # Polling endpoint
    â”‚   â”‚   â”‚   â”œâ”€â”€ sentiment-inference/  # Legacy sync endpoint
    â”‚   â”‚   â”‚   â”œâ”€â”€ upload-url/        # Presigned URL (deprecated)
    â”‚   â”‚   â”‚   â””â”€â”€ auth/              # NextAuth API routes
    â”‚   â”‚   â”œâ”€â”€ login/             # Login page
    â”‚   â”‚   â”œâ”€â”€ signup/            # Signup page
    â”‚   â”‚   â”œâ”€â”€ page.tsx           # Landing page (dashboard)
    â”‚   â”‚   â””â”€â”€ layout.tsx         # Root layout
    â”‚   â”‚
    â”‚   â”œâ”€â”€ components/            # React Components
    â”‚   â”‚   â”œâ”€â”€ client/            # Client components
    â”‚   â”‚   â”‚   â”œâ”€â”€ Inference.tsx      # Main analysis UI
    â”‚   â”‚   â”‚   â””â”€â”€ UploadVideo.tsx    # Upload + polling logic
    â”‚   â”‚   â””â”€â”€ ui/                # shadcn/ui components
    â”‚   â”‚
    â”‚   â”œâ”€â”€ server/                # Backend Config
    â”‚   â”‚   â”œâ”€â”€ auth/
    â”‚   â”‚   â”‚   â”œâ”€â”€ config.ts      # NextAuth configuration
    â”‚   â”‚   â”‚   â””â”€â”€ index.ts       # Auth exports
    â”‚   â”‚   â””â”€â”€ db.ts              # Prisma client
    â”‚   â”‚
    â”‚   â”œâ”€â”€ actions/               # Server actions
    â”‚   â”‚   â””â”€â”€ auth.ts
    â”‚   â”œâ”€â”€ schemas/               # Zod validation schemas
    â”‚   â”‚   â””â”€â”€ auth.ts
    â”‚   â”œâ”€â”€ styles/
    â”‚   â”‚   â””â”€â”€ globals.css
    â”‚   â”œâ”€â”€ env.js                 # Environment validation
    â”‚   â””â”€â”€ middleware.ts          # Auth middleware
    â”‚
    â”œâ”€â”€ prisma/
    â”‚   â””â”€â”€ schema.prisma          # Database schema
    â”œâ”€â”€ generated/                 # Prisma generated client
    â”œâ”€â”€ public/                    # Static assets
    â””â”€â”€ package.json
```

## ğŸ”§ Setup & Deployment

### 1. Clone Repository

```bash
git clone https://github.com/yourusername/video-sentiment-saas.git
cd video-sentiment-saas
```

### 2. Install Dependencies

```bash
# SaaS app
cd video-sentiment-saas
npm install

# ML training (optional - if you want to train from scratch)
cd ../sentiment-model/training
pip install -r requirements.txt
```

### 3. Environment Variables

Create `video-sentiment-saas/.env` file:

```env
# Database
DATABASE_URL="postgresql://..."

# NextAuth
NEXTAUTH_SECRET="your-secret-key"
NEXTAUTH_URL="http://localhost:3000"

# AWS Credentials
AWS_ACCESS_KEY_ID="..."
AWS_SECRET_ACCESS_KEY="..."
AWS_REGION="us-east-1"

# S3 Configuration
S3_BUCKET_NAME="your-video-bucket"

# SageMaker
SAGEMAKER_ENDPOINT_NAME="sentiment-endpoint"
```

### 4. Database Setup

```bash
cd video-sentiment-saas
npx prisma db push
npx prisma generate
```

### 5. Deploy SageMaker Endpoint

```bash
cd sentiment-model/deployment

# Package model code (already done - model.tar.gz exists)
# If you need to repackage:
# tar czf model.tar.gz inference.py models.py requirements.txt

# Upload to S3
aws s3 cp model.tar.gz s3://your-sagemaker-bucket/models/

# Create SageMaker model
aws sagemaker create-model \
  --model-name sentiment-analysis-model \
  --primary-container \
    Image=763104351884.dkr.ecr.us-east-1.amazonaws.com/pytorch-inference:2.0.0-gpu-py310,\
    ModelDataUrl=s3://your-sagemaker-bucket/models/model.tar.gz \
  --execution-role-arn arn:aws:iam::YOUR_ACCOUNT:role/SageMakerExecutionRole

# Create endpoint configuration
aws sagemaker create-endpoint-config \
  --endpoint-config-name sentiment-endpoint-config \
  --production-variants \
    VariantName=AllTraffic,\
    ModelName=sentiment-analysis-model,\
    InstanceType=ml.g5.xlarge,\
    InitialInstanceCount=1

# Create endpoint (takes ~5-10 minutes)
aws sagemaker create-endpoint \
  --endpoint-name sentiment-endpoint \
  --endpoint-config-name sentiment-endpoint-config

# Check status
aws sagemaker describe-endpoint --endpoint-name sentiment-endpoint
```

**Alternative: Use Python scripts**

```bash
cd sentiment-model/deployment

# Package and upload
python create_model_package.py

# Deploy endpoint
python deploy_endpoint.py
```

### 6. Run Development Server

```bash
cd video-sentiment-saas
npm run dev
```

Visit `http://localhost:3000`

## ğŸ“ How It Works

### Inference Pipeline

```
Video Upload â†’ S3 Storage
     â†“
Database Record Created
     â†“
Async Analysis Started (returns immediately)
     â†“
SageMaker Endpoint:
  1. Whisper transcribes audio â†’ utterances
  2. For each utterance:
     - RoBERTa â†’ emotion scores (7 classes)
     - DistilBERT â†’ sentiment scores (3 classes)
     â†“
Results saved to database
     â†“
Frontend polls every 5s â†’ displays results
```

### Key Components

**Async Processing Architecture:**
- `start-analysis` route triggers SageMaker but returns immediately
- Analysis runs in background (can take 60-90 seconds)
- `analysis-status` route polls for completion
- Frontend polls every 5 seconds with visual progress indicator

**SageMaker inference.py:**
1. Downloads video from S3
2. Validates file size (detects corrupted uploads)
3. Uses Whisper for speech-to-text with timestamps
4. Runs emotion + sentiment models on each utterance
5. Returns structured JSON with all scores

## ğŸ“Š Model Performance

| Metric | Score |
|--------|-------|
| Emotion Detection | 7 classes (anger, disgust, fear, joy, neutral, sadness, surprise) |
| Sentiment Detection | 3 classes (positive, negative, neutral) |
| Average Inference Time | 60-90 seconds (depends on video length) |
| Endpoint Instance | ml.g5.xlarge (GPU) |

## ğŸ› Debugging & Troubleshooting

**Common Issues Fixed During Development:**

### 1. 212-byte Corrupted Files in S3
**Problem:** Presigned URL uploads creating tiny corrupted files  
**Solution:** Switched to direct server-side upload using `PutObjectCommand`
```typescript
// âŒ Don't use presigned URLs with FormData
// âœ… Use direct server-side upload in /api/direct-upload
const command = new PutObjectCommand({
  Bucket: bucketName,
  Key: key,
  Body: buffer,
  ContentType: file.type,
});
```

### 2. SageMaker 60-Second Timeout
**Problem:** Inference takes 60-90 seconds but API Gateway times out at 60s  
**Solution:** Implemented async processing with polling
```typescript
// âŒ Don't wait for SageMaker response synchronously
// âœ… Start analysis, return immediately, poll for results
POST /api/start-analysis â†’ returns fileId immediately
GET /api/analysis-status?fileId=... â†’ poll every 5s
```

### 3. Database ID Mismatch
**Problem:** UUID generated for S3 key but Prisma auto-generates different ID  
**Solution:** Use same UUID for both S3 and database
```typescript
const id = crypto.randomUUID();
const key = `inference/${id}.mp4`;
await db.videoFile.create({
  data: { id: id, key: key, ... } // â† Pass explicit ID
});
```

### 4. JSON Parse Errors on Utterances
**Problem:** `JSON.parse("[object Object]")` error  
**Solution:** Prisma auto-parses JSON fields, don't parse again
```typescript
// âŒ Don't double-parse
emotions: JSON.parse(u.emotions)

// âœ… Use directly
emotions: u.emotions  // Already an object
```


## ğŸ“š Credits & Acknowledgments

**Built following:**
- [Andreas Trolle's tutorial](https://www.youtube.com/watch?v=Myo5kizoSk0) - Train & Deploy Multimodal AI

**Datasets & Models:**
- [MELD Dataset](https://affective-meld.github.io/) - Multimodal EmotionLines Dataset (Friends TV series)
- Whisper (OpenAI) - Speech transcription
- RoBERTa (Hugging Face) - Emotion classification
- DistilBERT (Hugging Face) - Sentiment classification

**Technologies:**
- PyTorch & Hugging Face Transformers
- AWS SageMaker for GPU training & deployment
- Next.js & Vercel
- shadcn/ui components


**Built with â¤ï¸ by [Your Name]**

#MachineLearning #SageMaker #NextJS #PyTorch #SaaS #BuildInPublic
